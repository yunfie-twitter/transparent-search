# Crawl Worker ã¨ Job ãƒ•ãƒ­ãƒ¼

## ğŸ”„ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ãƒ•ãƒ­ãƒ¼

```
èµ·å‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ startup.py (Pre-startup åˆæœŸåŒ–)                         â”‚
â”‚ - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–                                    â”‚
â”‚ - Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¥ç¶š                                  â”‚
â”‚ - Pending ã‚¸ãƒ§ãƒ–ã‚’ç¢ºèª                                  â”‚
â”‚ - ãƒ†ã‚¹ãƒˆã‚¸ãƒ§ãƒ–ä½œæˆï¼ˆå¿…è¦ãªå ´åˆï¼‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ main.py - Uvicorn èµ·å‹•                                  â”‚
â”‚ startup event ãƒˆãƒªã‚¬ãƒ¼                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ main.py - Startup Event Handler                         â”‚
â”‚ - DB ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç¢ºèª                                   â”‚
â”‚ - Redis æ¥ç¶šç¢ºèª                                        â”‚
â”‚ - Pending ã‚¸ãƒ§ãƒ–ã®æœ€çµ‚ç¢ºèª                              â”‚
â”‚ - crawl_worker.worker_loop() ã‚’ asyncio.Task ã§èµ·å‹•    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
âœ… API ã‚µãƒ¼ãƒãƒ¼èµ·å‹•å®Œäº†
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ crawl_worker.worker_loop() - è‡ªå‹•ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹       â”‚
â”‚ 5ç§’ã”ã¨ï¼špending ã‚¸ãƒ§ãƒ–ã‚’ãƒã‚§ãƒƒã‚¯                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Job ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é·ç§»

```
ã‚¸ãƒ§ãƒ–ä½œæˆï¼ˆapi/crawl/start ãªã©ï¼‰
        â†“
   pending
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker ãŒ pickup ã—ã€å‡¦ç†é–‹å§‹            â”‚
â”‚ - max_concurrent_jobs ã‚’ç¢ºèª            â”‚
â”‚ - ä¸¦è¡Œæ•° < max ã®å ´åˆã€job ã‚’å–å¾—       â”‚
â”‚ - status ã‚’ã€Œprocessingã€ã«æ›´æ–°         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
 processing ï¼ˆå®Ÿéš›ã®ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œä¸­ï¼‰
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æˆåŠŸ            â”‚    å¤±æ•—          â”‚
â”‚                  â”‚                  â”‚
v                  v                  v
completed       failed         â³ ãƒªãƒˆãƒ©ã‚¤
                                     â†“
                               pending
                                     â†“
                              [å†åº¦å‡¦ç†]
```

## ğŸš€ Pending â†’ Auto Processing ãƒ•ãƒ­ãƒ¼

### ã‚·ãƒŠãƒªã‚ª 1: startup.py ã§ãƒ†ã‚¹ãƒˆã‚¸ãƒ§ãƒ–ä½œæˆ

```bash
# 1. ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•
docker-compose up

# ãƒ­ã‚°å‡ºåŠ›:
# startup.py:
# 2026-01-11 14:31:02 - ğŸ¤“ Creating test crawl session and jobs...
# 2026-01-11 14:31:02 - âœ… Created session: 32e6f70d-75b6-4d5c-8b3a-c7dca0f246ff
# 2026-01-11 14:31:02 - âœ… Created job: 49220cac-ae3e-486b-9e69-1c8c6afd18fc
# 2026-01-11 14:31:02 - ğŸ”„ Will process 1 pending job(s) when worker starts

# main.py startup event:
# 2026-01-11 14:31:03 - ğŸ¤– Starting crawl worker...
# 2026-01-11 14:31:03 - ğŸš€ Crawl worker started (max_concurrent=3, poll_interval=5s)

# main.py health check (ç´„5ç§’å¾Œ):
# 2026-01-11 14:31:08 - ğŸ“¬ Found 1 pending jobs (available slots: 3)
# 2026-01-11 14:31:08 - ğŸ“¥ Starting 1 jobs (active: 0/3)
# 2026-01-11 14:31:08 - ğŸ”„ Processing job 49220cac: https://momon-ga.com (depth: 0)
```

### ã‚·ãƒŠãƒªã‚ª 2: API çµŒç”±ã§ã‚¸ãƒ§ãƒ–ä½œæˆ

```bash
# POST /api/crawl/start
curl -X POST "http://localhost:8080/api/crawl/start?domain=example.com&page_limit=50"

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹:
# {
#   "status": "success",
#   "session_id": "abc123...",
#   "domain": "example.com",
#   "configuration": { "page_limit": 50, "max_depth": 3 }
# }

# â†’ Job ãŒ pending çŠ¶æ…‹ã§ DB ã«ä¿å­˜ã•ã‚Œã‚‹
# â†’ Worker ã®æ¬¡ã®ãƒãƒ¼ãƒªãƒ³ã‚°å‘¨æœŸï¼ˆ5ç§’ä»¥å†…ï¼‰ã§è‡ªå‹•çš„ã«å‡¦ç†é–‹å§‹

# ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹:
# 2026-01-11 14:31:13 - ğŸ“¬ Found 1 pending jobs (available slots: 3)
# 2026-01-11 14:31:13 - ğŸ“¥ Starting 1 jobs (active: 0/3)
# 2026-01-11 14:31:13 - ğŸ”„ Processing job abc123de: https://example.com (depth: 0)
```

## ğŸ¯ ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‹•ä½œè©³ç´°

### ãƒãƒ¼ãƒªãƒ³ã‚°ã‚µã‚¤ã‚¯ãƒ«

```python
while worker.is_running:
    # 1. åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ­ãƒƒãƒˆæ•°ã‚’ç¢ºèª
    available_slots = max_concurrent_jobs - len(active_jobs)
    
    if available_slots > 0:
        # 2. åˆ©ç”¨å¯èƒ½ãªæ•°ã ã‘ pending ã‚¸ãƒ§ãƒ–ã‚’å–å¾—
        pending_jobs = get_pending_jobs(limit=available_slots)
        
        if pending_jobs:
            # 3. å„ã‚¸ãƒ§ãƒ–ã‚’ã‚¿ã‚¹ã‚¯åŒ–ã—ã¦ä¸¦è¡Œå‡¦ç†
            for job in pending_jobs:
                task = asyncio.create_task(process_job(job))
                active_jobs[job.job_id] = task
        else:
            # 4. Pending ã‚¸ãƒ§ãƒ–ãªã— â†’ ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ãƒãƒ¼ãƒªãƒ³ã‚°
            adaptive_poll_interval += 2  # æœ€å¤§30ç§’ã¾ã§å¢—åŠ 
    
    # 5. å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    completed = [job_id for job_id, task in active_jobs.items() if task.done()]
    for job_id in completed:
        del active_jobs[job_id]
    
    # 6. ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”ã¾ã§å¾…æ©Ÿ
    await asyncio.sleep(adaptive_poll_interval)
```

### ã‚¸ãƒ§ãƒ–å‡¦ç†ãƒ•ãƒ­ãƒ¼

```python
async def process_job(job: CrawlJob) -> bool:
    # 1. å®Ÿéš›ã®ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
    result = await crawler_service.execute_crawl_job(...)
    
    if result:
        # 2. æŠ½å‡ºã•ã‚ŒãŸ URL ã‚’ãƒã‚§ãƒƒã‚¯
        urls_to_crawl = result.get("urls_to_crawl", [])
        
        if urls_to_crawl and job.depth < job.max_depth:
            # 3. æ¬¡ã®æ·±åº¦ã®ã‚¸ãƒ§ãƒ–ã‚’è‡ªå‹•ä½œæˆ
            await queue_child_jobs(
                session_id=job.session_id,
                depth=job.depth + 1,
                urls=urls_to_crawl
            )
        
        # 4. ã‚¸ãƒ§ãƒ–ã‚’ completed ã«ãƒãƒ¼ã‚¯
        return True
    else:
        return False
```

## ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### Worker Status ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

```bash
curl http://localhost:8080/api/crawl/worker/status | jq .
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹**:
```json
{
  "status": "success",
  "worker": {
    "is_running": true,
    "active_jobs": 2,
    "available_slots": 1,
    "max_concurrent_jobs": 3,
    "poll_interval": 5,
    "global_queue": {
      "pending": 15,
      "processing": 2
    },
    "metrics": {
      "total_processed": 45,
      "total_successful": 43,
      "total_failed": 2,
      "total_queued": 187,
      "success_rate": "95.6%",
      "avg_job_time_ms": "1245ms",
      "uptime_seconds": "234.5s"
    }
  }
}
```

### Session Statistics ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

```bash
curl "http://localhost:8080/api/crawl/worker/session/{session_id}" | jq .
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹**:
```json
{
  "status": "success",
  "session": {
    "session_id": "abc123...",
    "domain": "example.com",
    "status": "active",
    "progress": "42.5%",
    "total_jobs": 40,
    "completed_jobs": 17,
    "pending_jobs": 18,
    "processing_jobs": 5,
    "failed_jobs": 0,
    "avg_depth": 1.2
  }
}
```

## ğŸ”Œ è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### crawl_worker ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š

```python
crawl_worker = CrawlWorker(
    max_concurrent_jobs=3,      # åŒæ™‚å‡¦ç†ã‚¸ãƒ§ãƒ–æ•°
    poll_interval=5             # ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”ï¼ˆç§’ï¼‰
)
```

### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# åŒæ™‚å‡¦ç†ã‚¸ãƒ§ãƒ–æ•°ã‚’å¢—ã‚„ã™
crawl_worker.max_concurrent_jobs = 5

# ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”ã‚’çŸ­ãã™ã‚‹
crawl_worker.poll_interval = 2
```

## âš ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¸ãƒ§ãƒ–ãŒå‡¦ç†ã•ã‚Œãªã„

1. **Worker ãŒå®Ÿè¡Œä¸­ã‹ç¢ºèª**
   ```bash
   curl http://localhost:8080/health | jq '.components.crawl_worker'
   # ã€Œoperationalã€ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
   ```

2. **Pending ã‚¸ãƒ§ãƒ–ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª**
   ```bash
   curl http://localhost:8080/health | jq '.database_stats'
   # pending > 0 ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
   ```

3. **Worker ãƒ­ã‚°ã‚’ç¢ºèª**
   ```bash
   docker-compose logs -f transparent_search_app | grep "crawl_worker"
   ```

### å‡¦ç†ãŒé…ã„

1. **åŒæ™‚å‡¦ç†æ•°ã‚’å¢—ã‚„ã™**
   ```python
   # max_concurrent_jobs ã‚’å¢—ã‚„ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
   ```

2. **ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”ã‚’çŸ­ãã™ã‚‹**
   ```python
   # poll_interval ã‚’æ¸›ã‚‰ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ç§’ï¼‰
   ```

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„

1. **åŒæ™‚å‡¦ç†æ•°ã‚’æ¸›ã‚‰ã™**
   ```python
   # max_concurrent_jobs ã‚’æ¸›ã‚‰ã™
   ```

2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢**
   ```bash
   curl -X POST "http://localhost:8080/api/crawl/invalidate?domain=example.com"
   ```

## ğŸ“ ã¾ã¨ã‚

**è‡ªå‹•å‡¦ç†ãƒ•ãƒ­ãƒ¼:**

1. âœ… `startup.py` â†’ ãƒ†ã‚¹ãƒˆã‚¸ãƒ§ãƒ–ä½œæˆï¼ˆpending çŠ¶æ…‹ï¼‰
2. âœ… `main.py` â†’ Uvicorn èµ·å‹• + Worker èµ·å‹•
3. âœ… `crawl_worker.worker_loop()` â†’ 5ç§’ã”ã¨ã«ãƒãƒ¼ãƒªãƒ³ã‚°
4. âœ… Pending ã‚¸ãƒ§ãƒ–ã‚’è‡ªå‹•æ¤œå‡º â†’ å‡¦ç†é–‹å§‹
5. âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº† â†’ å­ã‚¸ãƒ§ãƒ–ä½œæˆ
6. âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–° â†’ API ã§ç›£è¦–å¯èƒ½

**è¿½åŠ ã‚¸ãƒ§ãƒ–ã®å‡¦ç†ã‚‚åŒã˜æµã‚Œ:**

- API ã§ `/api/crawl/start` ã‚’å©ã
  â†“
- Job ãŒ pending ã§ä¿å­˜ã•ã‚Œã‚‹
  â†“
- Worker ãŒæ¬¡ã®ãƒãƒ¼ãƒªãƒ³ã‚°å‘¨æœŸã§è‡ªå‹•å‡¦ç†
  â†“
- `/api/crawl/worker/status` ã§é€²æ—ç›£è¦–å¯èƒ½
