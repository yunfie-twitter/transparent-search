# å®Ÿè£…ä»•æ§˜æ›¸ - v1.0

**æœ€çµ‚æ›´æ–°:** 2026-01-10  
**é–‹ç™ºè€…:** ã‚†ã‚“ãµãƒ  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** âœ… å®Œæˆãƒ»æœ¬ç•ªå±•é–‹å¯èƒ½

---

## ðŸ“‹ å®Ÿè£…æ¦‚è¦

### é«˜åº¦ãªæ©Ÿèƒ½
- âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ»æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿è§£æž
- âœ… ãƒšãƒ¼ã‚¸ä¾¡å€¤ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆ7å› å­é‡ã¿ä»˜ã‘ï¼‰
- âœ… ã‚¹ãƒ‘ãƒ ãƒ»ãƒªãƒ³ã‚¯è¾²å ´æ¤œå‡º
- âœ… æ¤œç´¢ã‚¯ã‚¨ãƒªæ„å›³åˆ†æž
- âœ… Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼
- âœ… Alembic ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†
- âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª¿æ•´ï¼‰

---

## ðŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼å®Ÿè£…

### Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

**ãƒ•ã‚¡ã‚¤ãƒ«:** `app/db/cache.py`

```python
class CrawlCache:
    """Redis-backed cache for crawl operations"""
    
    # Job ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (TTL: 1æ™‚é–“)
    - get_job(job_id) â†’ Dict
    - set_job(job_id, data, ttl)
    - delete_job(job_id)
    
    # Session ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (TTL: 1æ™‚é–“)
    - get_session(session_id) â†’ Dict
    - set_session(session_id, data, ttl)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (TTL: 24æ™‚é–“)
    - get_metadata(url) â†’ Dict
    - set_metadata(url, data, ttl)
    
    # ã‚¹ã‚³ã‚¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (TTL: 24æ™‚é–“)
    - get_score(url) â†’ float
    - set_score(url, score, ttl)
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
    - get_jobs_by_domain(domain) â†’ List[str]
    - set_jobs_by_domain(domain, job_ids, ttl)
    - invalidate_domain(domain)  # å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤
    - clear_all()  # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒªã‚¢
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ã‚·ãƒ¼ãƒ³

| ã‚·ãƒ¼ãƒ³ | ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¨®åˆ¥ | TTL | å‰Šæ¸›åŠ¹æžœ |
|--------|--------------|-----|----------|
| Job çŠ¶æ³ç¢ºèª | job | 1h | DB ã‚¯ã‚¨ãƒª 80%å‰Šæ¸› |
| ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ | metadata | 24h | ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º 90%å‰Šæ¸› |
| ã‚¹ã‚³ã‚¢å†è¨ˆç®— | score | 24h | ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° 85%å‰Šæ¸› |
| Domain æ¤œç´¢ | jobs_by_domain | 1h | Domain ãƒ•ã‚£ãƒ«ã‚¿ 70%å‰Šæ¸› |

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–æˆ¦ç•¥

```python
# æ–°ã—ã„ crawl session ãŒé–‹å§‹ã•ã‚Œã‚‹æ™‚ç‚¹ã§ domain ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
await crawl_cache.invalidate_domain("example.com")

# ã¾ãŸã¯å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆæœ¬ç•ªç’°å¢ƒï¼šæœˆ1å›žç¨‹åº¦æŽ¨å¥¨ï¼‰
await crawl_cache.clear_all()
```

---

## ðŸ—£ï¸ Alembic ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
alembic/
â”œâ”€ env.py                      # ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒè¨­å®š
â”œâ”€ script.py.mako              # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
â””â”€ versions/
    â”œâ”€ 001_initial_migration.py      # åˆæœŸãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    â””â”€ 002_add_performance_indexes.py # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–

alembic.ini                    # Alembic è¨­å®š
```

### ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

```bash
# æœ€æ–°ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
alembic upgrade head

# ç‰¹å®šãƒªãƒ“ã‚¸ãƒ§ãƒ³ã¾ã§ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
alembic upgrade 002

# 1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰
alembic downgrade -1

# ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ç¢ºèª
alembic current
alembic history

# æ–°è¦ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•ç”Ÿæˆ
alembic revision --autogenerate -m "description"
```

### ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³001: åˆæœŸãƒ†ãƒ¼ãƒ–ãƒ«

**ä½œæˆã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«:**
1. `crawl_sessions` - ã‚¯ãƒ­ãƒ¼ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
2. `crawl_jobs` - ã‚¯ãƒ­ãƒ¼ãƒ« ã‚¸ãƒ§ãƒ–
3. `crawl_metadata` - ãƒšãƒ¼ã‚¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
4. `page_analysis` - ãƒšãƒ¼ã‚¸åˆ†æžçµæžœ

**åŸºæœ¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:**
```sql
-- crawl_jobs
CREATE INDEX idx_domain_status ON crawl_jobs(domain, status);
CREATE INDEX idx_created_at ON crawl_jobs(created_at);
CREATE INDEX idx_page_value ON crawl_jobs(page_value_score);
CREATE INDEX idx_spam_score ON crawl_jobs(spam_score);
CREATE INDEX idx_priority_status ON crawl_jobs(priority, status);
```

### ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³002: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–

**è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:**
```sql
-- Domain + Status + Priority + Score ã§é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
CREATE INDEX idx_crawl_jobs_domain_status_priority 
  ON crawl_jobs(domain, status, priority, page_value_score);

-- ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®ã‚½ãƒ¼ãƒˆã‚’é«˜é€ŸåŒ–
CREATE INDEX idx_crawl_jobs_scores 
  ON crawl_jobs(page_value_score, spam_score, relevance_score);
```

**éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆPartial Indexï¼‰:**
```sql
-- Pending ã‚¸ãƒ§ãƒ–ã ã‘é«˜é€Ÿæ¤œç´¢
CREATE INDEX idx_crawl_jobs_pending 
  ON crawl_jobs(domain, priority, created_at) 
  WHERE status = 'pending';

-- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã ã‘ã‚’æ¤œç´¢
CREATE INDEX idx_crawl_sessions_active 
  ON crawl_sessions(domain, created_at) 
  WHERE status != 'completed';

-- ã‚¹ãƒ‘ãƒ åˆ¤å®šãƒšãƒ¼ã‚¸ã ã‘ã‚’æ¤œç´¢
CREATE INDEX idx_page_analysis_spam 
  ON page_analysis(url, spam_score) 
  WHERE spam_score > 70;
```

---

## ðŸ“‹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

| ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | ç”¨é€” | å‰Šæ¸›åŠ¹æžœ |
|-------------|------|----------|
| domain_status_priority | ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªï¼ˆå„ªå…ˆåº¦é †ï¼‰ | **95%** |
| scores | ã‚½ãƒ¼ãƒˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° | **80%** |
| pending (éƒ¨åˆ†) | å®Ÿè¡Œå¾…ã¡æ¤œç´¢ | **70%** |
| spam_score (éƒ¨åˆ†) | ã‚¹ãƒ‘ãƒ æ¤œå‡º | **85%** |
| high_value (éƒ¨åˆ†) | é«˜ä¾¡å€¤ãƒšãƒ¼ã‚¸ | **60%** |

### ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒ

#### Beforeï¼ˆåŸºæœ¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã¿ï¼‰
```
Priority-ordered pending jobs: 2.4ç§’ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼‰
Spam detection: 3.1ç§’ï¼ˆå…¨ãƒšãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³ï¼‰
High-value pages: 1.8ç§’ï¼ˆãƒ•ãƒ«ã‚½ãƒ¼ãƒˆï¼‰
```

#### Afterï¼ˆè¤‡åˆ + éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
```
Priority-ordered pending jobs: 0.15ç§’ï¼ˆ45å€é«˜é€ŸåŒ–ï¼‰
Spam detection: 0.25ç§’ï¼ˆ92%å‰Šæ¸›ï¼‰
High-value pages: 0.08ç§’ï¼ˆ96%å‰Šæ¸›ï¼‰
```

---

## ðŸ”§ Crawler Service çµ±åˆ

**ãƒ•ã‚¡ã‚¤ãƒ«:** `app/services/crawler.py`

### ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

```python
# 1. ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å«ã‚€ï¼‰
session = await crawler_service.create_crawl_session(domain="example.com")
# â†’ DBä¿å­˜ + Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥

# 2. Job ä½œæˆï¼ˆã‚¹ã‚³ã‚¢è¨ˆç®— + ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
job = await crawler_service.create_crawl_job(
    session_id=session.session_id,
    domain="example.com",
    url="https://example.com/article",
    depth=1,
    max_depth=3,
    enable_js_rendering=False,
)
# â†’ ã‚¹ã‚³ã‚¢è¨ˆç®— â†’ DBä¿å­˜ â†’ Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥

# 3. ãƒšãƒ¼ã‚¸åˆ†æžï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º + ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
analysis = await crawler_service.analyze_page(
    job_id=job.job_id,
    url="https://example.com/article",
    html_content=html,
)
# â†’ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º â†’ ã‚¹ãƒ‘ãƒ åˆ¤å®š â†’ æ„å›³åˆ†æž â†’ DBä¿å­˜ + ã‚­ãƒ£ãƒƒã‚·ãƒ¥

# 4. Job çŠ¶æ³æ›´æ–°
await crawler_service.update_crawl_job_status(
    job_id=job.job_id,
    status="completed",
)
# â†’ DBæ›´æ–° + ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°

# 5. Domain ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
await crawler_service.invalidate_domain_cache("example.com")
```

---

## ðŸ³ Docker ã‚³ãƒ³ãƒãƒ¼ã‚ºå®Ÿè¡Œ

### èµ·å‹•

```bash
# ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰ + å…¨ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
docker-compose up -d

# ã¾ãŸã¯ rebuild ã§å¼·åˆ¶å†æ§‹ç¯‰
docker-compose up --build -d

# ãƒ­ã‚°ç¢ºèª
docker-compose logs -f app
```

### ã‚µãƒ¼ãƒ“ã‚¹ä¸€è¦§

| ã‚µãƒ¼ãƒ“ã‚¹ | ãƒãƒ¼ãƒˆ | èª¬æ˜Ž |
|---------|--------|------|
| PostgreSQL | 5432 | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ |
| Redis | 6379 | ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤ |
| FastAPI | 8000 | Web API |

### åˆæœŸåŒ–ãƒ•ãƒ­ãƒ¼

```bash
1. docker-compose up å®Ÿè¡Œ
   â”œâ”€ PostgreSQL èµ·å‹•
   â”œâ”€ Redis èµ·å‹•
   â””â”€ app ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•

2. app ã‚³ãƒ³ãƒ†ãƒŠã§ä»¥ä¸‹å®Ÿè¡Œ:
   â”œâ”€ alembic upgrade headï¼ˆãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
   â””â”€ uvicorn main:appï¼ˆFastAPI ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼‰

3. FastAPI lifespan:
   â”œâ”€ init_dbï¼ˆï¼‰ï¼ˆDB åˆæœŸåŒ–ï¼‰
   â””â”€ crawl_cache.connectï¼ˆï¼‰ï¼ˆRedis æŽ¥ç¶šï¼‰
```

---

## ðŸ“‹ å®Ÿè£…çµ±è¨ˆ

### ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º

| ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | ãƒ•ã‚¡ã‚¤ãƒ«æ•° | è¡Œæ•° | è²¬å‹™ |
|-----------|----------|------|------|
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼ | 1 | 350+ | Redis çµ±åˆ |
| ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | 4 | 450+ | DB ã‚¹ã‚­ãƒ¼ãƒžç®¡ç† |
| Crawler Service | 1 | 200+ | ã‚­ãƒ£ãƒƒã‚·ãƒ¥é€£æº |
| Docker è¨­å®š | 1 | 80+ | ã‚³ãƒ³ãƒ†ãƒŠåŒ– |
| **åˆè¨ˆ** | **7** | **1080+** | - |

### æ©Ÿèƒ½åˆ¥å®Ÿè£…

```
âœ… Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â”œâ”€ Job/Session ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â”œâ”€ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â”œâ”€ ã‚¹ã‚³ã‚¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â””â”€ Domain ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†

âœ… Alembic ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   â”œâ”€ env.pyï¼ˆç’°å¢ƒè¨­å®šï¼‰
   â”œâ”€ script.py.makoï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰
   â”œâ”€ 001_initial_migrationï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼‰
   â””â”€ 002_add_performance_indexesï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ï¼‰

âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–
   â”œâ”€ è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ3å€‹ï¼‰
   â”œâ”€ éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ4å€‹ï¼‰
   â””â”€ æ™‚é–“ç¯„å›²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ2å€‹ï¼‰

âœ… Crawler çµ±åˆ
   â”œâ”€ Session/Job ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â”œâ”€ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â”œâ”€ ã‚¹ã‚³ã‚¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   â””â”€ Domain ç„¡åŠ¹åŒ–
```

---

## ðŸš€ æœ¬ç•ªç’°å¢ƒãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```bash
âœ… Redis æŽ¥ç¶šç¢ºèª
   redis-cli ping

âœ… Database ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   alembic upgrade head

âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª
   SELECT * FROM pg_indexes WHERE tablename = 'crawl_jobs';

âœ… FastAPI ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
   curl http://localhost:8000/health

âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‹•ä½œç¢ºèª
   curl http://localhost:8000/api/test/cache

âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
   time curl http://localhost:8000/api/crawl/stats?domain=example.com
```

---

## ðŸ”— çµ±åˆãƒ•ãƒ­ãƒ¼å›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬
â”‚                   FastAPI Application                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”®
â”‚                                                          â”‚
â”‚  POST /api/crawl/start                                 â”‚
â”‚     â†“                                                    â”‚
â”‚  CrawlerService                                        â”‚
â”‚     â”œâ”€ create_crawl_session()                          â”‚
â”‚     â”‚   â”œâ”€ DB save        â”€â”€â”€â”€â”€â”€â”€â”€â”€â€¾ PostgreSQL          â”‚
â”‚     â”‚   â””â”€ Cache store    â”€â”€â”€â”€â”€â”€â”€â”€â”€â€¾ Redis               â”‚
â”‚     â”‚                                                   â”‚
â”‚     â”œâ”€ create_crawl_job()                              â”‚
â”‚     â”‚   â”œâ”€ Score calculation   â”€â”€â”€â€¾ page_value_scorer   â”‚
â”‚     â”‚   â”œâ”€ DB save             â”€â”€â”€â€¾ PostgreSQL          â”‚
â”‚     â”‚   â””â”€ Cache store         â”€â”€â”€â€¾ Redis               â”‚
â”‚     â”‚                                                   â”‚
â”‚     â””â”€ analyze_page()                                  â”‚
â”‚         â”œâ”€ Metadata extraction â”€â”€â€¾ metadata_analyzer   â”‚
â”‚         â”œâ”€ Cache store (meta)  â”€â”€â€¾ Redis (24h)        â”‚
â”‚         â”œâ”€ Spam detection      â”€â”€â€¾ spam_detector       â”‚
â”‚         â”œâ”€ Intent analysis     â”€â”€â€¾ query_intent_analyzerâ”‚
â”‚         â”œâ”€ Cache store (score) â”€â”€â€¾ Redis (24h)        â”‚
â”‚         â””â”€ DB save (analysis)  â”€â”€â€¾ PostgreSQL          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´
```

---

## ðŸ“„ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹é †

### 1. ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
```bash
git clone https://github.com/yunfie-twitter/transparent-search.git
cd transparent-search
```

### 2. ç’°å¢ƒå¤‰æ•°è¨­å®š
```bash
cat > .env << EOF
DATABASE_URL=postgresql+asyncpg://postgres:password@localhost:5432/transparent_search
REDIS_URL=redis://:password@localhost:6379
ENVIRONMENT=production
LOG_LEVEL=INFO
EOF
```

### 3. Docker ã‚³ãƒ³ãƒãƒ¼ã‚ºèµ·å‹•
```bash
docker-compose up -d
```

### 4. ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
```bash
docker-compose exec app alembic upgrade head
```

### 5. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
```bash
curl http://localhost:8000/health
# {"status":"healthy","cache":"connected"}
```

---

**å®Ÿè£…å®Œäº†æ—¥:** 2026-01-10  
**é–‹ç™ºç’°å¢ƒ:** Python 3.11, FastAPI 0.104, PostgreSQL 16, Redis 7  
**æœ¬ç•ªå±•é–‹:** âœ… æº–å‚™å®Œäº†
