# ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œãƒ•ãƒ­ãƒ¼è¨ºæ–­ã‚¬ã‚¤ãƒ‰

## ğŸ”§ ã™ã¹ã¦ã®ä¿®æ­£å†…å®¹

### 1ï¸âƒ£ crawl_worker.py (ä¿®æ­£å®Œäº†)
- âœ… `process_job()` ã§ job status ã‚’ "processing" ã«æ›´æ–° + `db.commit()` è¿½åŠ 
- âœ… è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¿½åŠ 
- âœ… ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„

### 2ï¸âƒ£ crawler.py (ä¿®æ­£å®Œäº†)
- âœ… `execute_crawl_job()` ã§ `await db.commit()` ã‚’æ˜ç¤ºçš„ã«è¿½åŠ 
- âœ… `analyze_page()` ã§ analysis ä¿å­˜å¾Œã« `await db.commit()`
- âœ… `update_crawl_job_status()` ã§ status æ›´æ–°å¾Œã« `await db.commit()`

### 3ï¸âƒ£ main.py (ä¿®æ­£å®Œäº†)
- âœ… lifespan ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ Worker ã‚’çµ±åˆ
- âœ… startup ã§ Worker ã‚¿ã‚¹ã‚¯é–‹å§‹
- âœ… shutdown ã§ Worker åœæ­¢ã¨å¾…æ©Ÿ

### 4ï¸âƒ£ startup.py (ä¿®æ­£å®Œäº†)
- âœ… ãƒ†ã‚¹ãƒˆã‚¸ãƒ§ãƒ–ä½œæˆãƒ­ã‚¸ãƒƒã‚¯ç°¡æ½”åŒ–
- âœ… æ¡ä»¶åˆ†å²æ”¹å–„ (total == 0 ã®å ´åˆã®ã¿ä½œæˆ)

---

## ğŸš€ å®Ÿè¡Œãƒ•ãƒ­ãƒ¼

```
1. docker-compose up
   â†“
2. startup.py å®Ÿè¡Œ
   - DB åˆæœŸåŒ–
   - Redis æ¥ç¶š
   - Pending ã‚¸ãƒ§ãƒ–ã‚’ãƒã‚§ãƒƒã‚¯ (total == 0 ãªã‚‰ test job ä½œæˆ)
   â†“
3. Uvicorn (main.py) èµ·å‹•
   - lifespan startup é–‹å§‹
   - Worker.is_running = True
   - asyncio.create_task(worker_loop()) å®Ÿè¡Œ
   â†“
4. Worker ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹ (5ç§’é–“éš”)
   - pending jobs ã‚’ã‚¯ã‚¨ãƒª
   - åˆ©ç”¨å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆãŒã‚ã‚Œã° process_job() ã‚¿ã‚¹ã‚¯åŒ–
   â†“
5. Job å‡¦ç†
   - status â†’ "processing" + db.commit()
   - execute_crawl_job() å®Ÿè¡Œ
   - ãƒªãƒ³ã‚¯æŠ½å‡º â†’ å­ job ä½œæˆ (pending)
   - status â†’ "completed" + db.commit()
   â†“
6. API ã§ç›£è¦–
   - GET /health â†’ worker_status, active_jobs
   - GET /api/crawl/worker/status â†’ detailed metrics
   - GET /api/crawl/worker/session/{id} â†’ progress
```

---

## ğŸ” ãƒ‡ãƒãƒƒã‚°æ‰‹é †

### Step 1: ãƒ­ã‚°ç¢ºèª

```bash
docker-compose up
```

ãƒ­ã‚°ã‚’è¦‹ã¦:
```
âœ… Database initialized
âœ… Redis cache connected
ğŸ“ Creating test crawl session and jobs...
âœ… Created session: xxx
âœ… Created job: yyy for https://momon-ga.com
ğŸ“‹ Updated Job Stats: total=1, pending=1, processing=0, completed=0, failed=0
ğŸš€ Starting Transparent Search application...
ğŸ”§ Worker configuration: max_concurrent=3, poll_interval=5s
âœ… Crawl worker task created
```

ã“ã“ã¾ã§ã§ **Worker ãŒèµ·å‹•ã•ã‚Œã¦ã„ã‚‹** ã“ã¨ã‚’ç¢ºèª

### Step 2: Worker ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹ç¢ºèª (~5ç§’å¾Œ)

ãƒ­ã‚°ã§ä»¥ä¸‹ã‚’ç¢ºèª:
```
ğŸ“¬ Found 1 pending jobs (available slots: 3)
ğŸ“¥ Starting 1 jobs (active: 0/3)
ğŸ”„ Processing job xxxxxxxx: https://momon-ga.com (depth: 0)
```

### Step 3: ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œç¢ºèª

```
ğŸŒ [xxxxxxxx] Fetching: https://momon-ga.com
ğŸ” [xxxxxxxx] Extracting links from https://momon-ga.com
âœ… [xxxxxxxx] Filtered to 12 internal links
ğŸ“¬ [xxxxxxxx] Adding 12 URLs to pending queue...
âœ¨ [xxxxxxxx] Pending queue updated: 12 new jobs created
ğŸ‰ [xxxxxxxx] Completed: https://momon-ga.com
âœ… Job xxxxxxxx completed in 1245ms â†’ 12 URLs queued
```

### Step 4: API ã§ç¢ºèª

```bash
# Worker å…¨ä½“çŠ¶æ…‹
curl http://localhost:8080/api/crawl/worker/status | jq .

# æœŸå¾…å€¤:
{
  "status": "success",
  "worker": {
    "is_running": true,
    "active_jobs": 2,  # å‡¦ç†ä¸­ã®ã‚¸ãƒ§ãƒ–
    "available_slots": 1,
    "max_concurrent_jobs": 3,
    "global_queue": {
      "pending": 11,  # æ®‹ã‚Šã® pending jobs
      "processing": 2
    },
    "metrics": {
      "total_processed": 1,
      "total_successful": 1,
      "total_failed": 0,
      "total_queued": 12,
      "success_rate": "100.0%",
      "avg_job_time_ms": "1245ms",
      "uptime_seconds": "10.5s"
    }
  }
}
```

---

## âŒ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Issue 1: Pending ã‚¸ãƒ§ãƒ–ãŒå‡¦ç†ã•ã‚Œãªã„

**ç¢ºèªé …ç›®:**

```bash
# 1. Worker ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹
curl http://localhost:8080/health | jq '.worker'
# â†’ "operational" ã§ã‚ã‚‹ã“ã¨

# 2. Pending ã‚¸ãƒ§ãƒ–ãŒå­˜åœ¨ã™ã‚‹ã‹
curl http://localhost:8080/health | jq '.database_stats'
# â†’ pending > 0 ã§ã‚ã‚‹ã“ã¨

# 3. Worker ãƒ­ã‚°ã‚’ç¢ºèª
docker-compose logs -f transparent_search_app | grep "Found.*pending"
# â†’ "Found 1 pending jobs" ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã“ã¨
```

**è§£æ±ºæ–¹æ³•:**

1. **Worker ãŒèµ·å‹•ã—ã¦ã„ãªã„å ´åˆ:**
   ```bash
   docker-compose logs transparent_search_app | grep "Crawl worker"
   # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹ã‹ç¢ºèª
   ```

2. **Pending ã‚¸ãƒ§ãƒ–ãŒãªã„å ´åˆ:**
   ```bash
   # API ã§æ–°ã—ã„ã‚¸ãƒ§ãƒ–ã‚’ä½œæˆ
   curl -X POST "http://localhost:8080/api/crawl/start?domain=example.com"
   ```

3. **5ç§’ä»¥ä¸Šå¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒ­ã‚°ç¢ºèª**
   - ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”ãŒ 5 ç§’ã®ãŸã‚ã€èµ·å‹•å¾Œ 5 ç§’ä»¥ä¸ŠçµŒéãŒå¿…è¦

### Issue 2: Job ãŒ pending ã®ã¾ã¾

```bash
# DB ã«ç›´æ¥å•ã„åˆã‚ã›
docker exec transparent_search_postgres psql -U app_user -d transparent_search -c \
  "SELECT job_id, status, url FROM crawl_job LIMIT 10;"

# æœŸå¾…å€¤: status ãŒ pending ã‹ã‚‰ completed ã«å¤‰ã‚ã‚‹
```

**åŸå› :**
- âŒ Worker ãŒå®Ÿéš›ã«å®Ÿè¡Œã•ã‚Œã¦ã„ãªã„
- âŒ HTTP é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ (DNS è§£æ±ºå¤±æ•—ãªã©)
- âŒ DB commit ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„

**è§£æ±º:**
```bash
# ãƒ­ã‚°ã§è©³ç´°ç¢ºèª
docker-compose logs -f transparent_search_app | grep -E "Error|Failed|âŒ"

# Worker å†èµ·å‹•
docker-compose restart transparent_search_app
```

### Issue 3: "completed" ãªã®ã«å­ã‚¸ãƒ§ãƒ–ãŒãªã„

**ç¢ºèª:**
```bash
# å­ã‚¸ãƒ§ãƒ–ãŒä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
docker exec transparent_search_postgres psql -U app_user -d transparent_search -c \
  "SELECT depth, COUNT(*) FROM crawl_job GROUP BY depth;"

# æœŸå¾…å€¤:
# depth | count
# ------|-------
#   0   |   1   (è¦ªã‚¸ãƒ§ãƒ–)
#   1   |  12   (å­ã‚¸ãƒ§ãƒ–)
```

**åŸå› :**
- max_depth ã«åˆ°é”ã—ã¦ã„ã‚‹
- ãƒªãƒ³ã‚¯æŠ½å‡ºã«å¤±æ•—ã—ã¦ã„ã‚‹

**ç¢ºèª:**
```bash
log | grep "Max depth\|Link extraction failed\|Filtered to 0"
```

### Issue 4: ãƒ¡ãƒ¢ãƒª/CPU ä½¿ç”¨ç‡ãŒé«˜ã„

**èª¿æ•´:**
```python
# crawl_worker.py
crawl_worker = CrawlWorker(
    max_concurrent_jobs=2,  # 3 ã‹ã‚‰ 2 ã«æ¸›ã‚‰ã™
    poll_interval=10        # 5 ã‹ã‚‰ 10 ã«å¢—ã‚„ã™
)
```

---

## ğŸ“Š ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–

```bash
# 5ç§’ã”ã¨ã«çŠ¶æ…‹è¡¨ç¤º
watch -n 5 'curl -s http://localhost:8080/health | jq '

# ã¾ãŸã¯
while true; do
  echo "=== $(date) ==="
  curl -s http://localhost:8080/api/crawl/worker/status | jq '.worker.metrics'
  sleep 5
done
```

### DB ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆç¢ºèª

```bash
# Job ä¸€è¦§
docker exec transparent_search_postgres psql -U app_user -d transparent_search -c \
  "SELECT status, COUNT(*) FROM crawl_job GROUP BY status;"

# ã‚»ãƒƒã‚·ãƒ§ãƒ³é€²æ—
docker exec transparent_search_postgres psql -U app_user -d transparent_search -c \
  "SELECT session_id, domain, created_at FROM crawl_session ORDER BY created_at DESC LIMIT 5;"

# æœ€æ–°ã®ã‚¸ãƒ§ãƒ–
docker exec transparent_search_postgres psql -U app_user -d transparent_search -c \
  "SELECT job_id, url, status, depth, created_at FROM crawl_job ORDER BY created_at DESC LIMIT 10;"
```

---

## âœ… æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ

### æ­£å¸¸ãªå ´åˆ

```
08:00:00 - App start
08:00:02 - Worker start
08:00:05 - First job picked (pending â†’ processing)
08:00:10 - Job completed (processing â†’ completed)
08:00:10 - Child jobs created (12 Ã— pending)
08:00:15 - Child jobs start processing (pending â†’ processing)
08:00:20 - Child jobs completed (processing â†’ completed)
```

### å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æœŸå¾…å€¤

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | æœŸå¾…å€¤ |
|---|---|
| GET / | redis connected |
| GET /health | worker: operational |
| GET /admin | active_jobså¢—åŠ  |
| GET /api/crawl/worker/status | total_processed > 0 |
| GET /api/crawl/worker/session/{id} | progress å¢—åŠ  |

---

## ğŸ¯ Next Steps

1. âœ… `docker-compose down -v && docker-compose up`
2. âœ… ãƒ­ã‚°ã§ Worker èµ·å‹•ç¢ºèª
3. âœ… 5~10 ç§’å¾…æ©Ÿ
4. âœ… Job processing ãƒ­ã‚°ç¢ºèª
5. âœ… API ã§ progress ç¢ºèª
6. âœ… DB ã§ job status å¤‰åŒ–ç¢ºèª

ã“ã‚Œã§ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ãŒè‡ªå‹•åŒ–ã•ã‚Œã¾ã™! ğŸš€
